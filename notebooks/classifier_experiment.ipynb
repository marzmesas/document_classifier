{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels_and_text(df):\n",
    "    labels = [int(i[0]) for i in df['5485']]\n",
    "    df['labels'] = labels\n",
    "    df = df.rename({'5485': 'doc_text'}, axis=1)\n",
    "    df['doc_text'] = df['doc_text'].str[1:]\n",
    "    return df\n",
    "\n",
    "def prepare_data(df):\n",
    "    df = preprocess_labels_and_text(df)\n",
    "    X = df['doc_text']\n",
    "    y = df['labels']\n",
    "    return X, y, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF-IDF + CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_features_tfidf(X_train, y_train):\n",
    "    # Apply TF-IDF with a HIGH max_features (e.g., 10,000)\n",
    "    high_max_features = 10000\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=high_max_features)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Train a Random Forest Classifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Get Feature Importances\n",
    "    feature_importances = rf.feature_importances_\n",
    "\n",
    "    # Determine Optimal max_features (Keeping Top 90% of Importance)\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]  # Sort features by importance (descending)\n",
    "    cumulative_importance = np.cumsum(feature_importances[sorted_indices])  # Cumulative importance\n",
    "    best_max_features = np.argmax(cumulative_importance >= 0.90)  # Find cutoff for 90% importance\n",
    "    return best_max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_test_pred, average='weighted', zero_division=1),\n",
    "        \"Precision\": precision_score(y_test, y_test_pred, average='weighted', zero_division=1),\n",
    "        \"Recall\": recall_score(y_test, y_test_pred, average='weighted', zero_division=1)\n",
    "    }\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_test_pred, labels=model.classes_)\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    # disp.plot(cmap='Blues')\n",
    "    # plt.title(f\"Confusion matrix for {type(model).__name__}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # print(f\"\\nClassification report for {type(model).__name__}:\\n\")\n",
    "    # print(classification_report(y_test, y_test_pred, zero_division=1))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, models, names):\n",
    "    results = []\n",
    "    for name, model in zip(names, models):\n",
    "        print(f\"Training and evaluating: {name}\")\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "        results.append({\"Classifier\": name, **metrics})\n",
    "        # print(f\"Results for {name}: {metrics}\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(results):\n",
    "    results.sort_values(by=\"Accuracy\", ascending=False, inplace=True)\n",
    "    plt.bar(results[\"Classifier\"], results[\"Accuracy\"])\n",
    "    plt.title(\"Model comparison\")\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run classification comparison with Lazypredict\n",
    "# clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "# models, predictions = clf.fit(X_train_tfidf.toarray(), X_test_tfidf.toarray(), y_train, y_test)\n",
    "\n",
    "# # Display model performance comparison\n",
    "# print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset and preprocess it\n",
    "path_csv = '../data/file.txt'\n",
    "df = pd.read_csv(path_csv)\n",
    "X, y, df = prepare_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Best max_features: 1175\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get best max_features value   \n",
    "best_max_features = get_max_features_tfidf(X_train, y_train)\n",
    "\n",
    "# Use the best max_features value for final TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=best_max_features)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Selected Best max_features: {best_max_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating: Logistic Regression\n",
      "Training and evaluating: KNN Classifier\n",
      "Training and evaluating: Decision Tree\n",
      "Training and evaluating: Linear SVM\n",
      "Training and evaluating: Random Forest\n",
      "Training and evaluating: SGD Classifier\n",
      "Training and evaluating: Ridge Classifier\n",
      "Training and evaluating: XGBoost\n",
      "Training and evaluating: AdaBoost\n"
     ]
    }
   ],
   "source": [
    "# Model Zoo\n",
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"KNN Classifier\",\n",
    "    \"Decision Tree\",\n",
    "    \"Linear SVM\",\n",
    "    \"Random Forest\",\n",
    "    \"SGD Classifier\",\n",
    "    \"Ridge Classifier\",\n",
    "    \"XGBoost\",\n",
    "    \"AdaBoost\",\n",
    "]\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    KNeighborsClassifier(n_neighbors=149, n_jobs=-1),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(kernel='linear'),\n",
    "    RandomForestClassifier(n_estimators=100, class_weight='balanced'),\n",
    "    SGDClassifier(loss='hinge'),\n",
    "    RidgeClassifier(),\n",
    "    XGBClassifier(),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "# Training and evaluation\n",
    "results = train_and_evaluate_models(X_train_tfidf, y_train, X_test_tfidf, y_test, models, names)\n",
    "\n",
    "# Result plotting\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  Accuracy  F1 Score  Precision  Recall\n",
       "0  Logistic Regression      0.95      0.95       0.96    0.95\n",
       "1       KNN Classifier      0.91      0.90       0.91    0.91\n",
       "2        Decision Tree      0.88      0.88       0.88    0.88\n",
       "3           Linear SVM      0.97      0.97       0.97    0.97\n",
       "4        Random Forest      0.94      0.94       0.95    0.94\n",
       "5       SGD Classifier      0.97      0.97       0.97    0.97\n",
       "6     Ridge Classifier      0.97      0.97       0.97    0.97\n",
       "7              XGBoost      0.95      0.95       0.95    0.95\n",
       "8             AdaBoost      0.75      0.73       0.84    0.75"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RoBERTa + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMLP(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(TransformerMLP, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),  # Binary classification (sigmoid output)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = output.last_hidden_state[:, 0, :]  # Extract [CLS] token embedding\n",
    "        return self.fc(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device).unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)  # Now both have shape [batch_size, 1]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tqdm description with loss\n",
    "            loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    loop = tqdm(test_loader, desc=\"Evaluating\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = (outputs.cpu().numpy() > 0.5).astype(int).flatten()\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = '../data/file.txt'\n",
    "df = pd.read_csv(path_csv)\n",
    "X, y, df = prepare_data(df)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"  # Change to \"distilbert-base-uncased\" for DistilBERT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.tolist(), y.tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, loss function & optimizer\n",
    "device = torch.device(\"cpu\")\n",
    "model = TransformerMLP(MODEL_NAME).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Train and evaluate\n",
    "train(model, train_loader)\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
